{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8x/sy02cz7j5z3310hqp4hcxkgh0000gn/T/ipykernel_17992/1078703302.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
      "/var/folders/8x/sy02cz7j5z3310hqp4hcxkgh0000gn/T/ipykernel_17992/1078703302.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"../data\", max_train=10000, max_test=1000)\n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "<img alt=\"image\" height=\"300\" src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3\" width=\"200\"/>\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "<img alt=\"image\" src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293\"/>\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "5.006760443547122"
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8x/sy02cz7j5z3310hqp4hcxkgh0000gn/T/ipykernel_17992/979710720.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [
    "from importlib import reload"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'linear_classifer' from '/Users/sashamalakhatka/Documents/ML/assignments/assignment1/linear_classifer.py'>"
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(linear_classifer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradients are different at (0, 2). Analytic: -0.32202, Numeric: -0.09980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8x/sy02cz7j5z3310hqp4hcxkgh0000gn/T/ipykernel_17992/3522664596.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
      "/var/folders/8x/sy02cz7j5z3310hqp4hcxkgh0000gn/T/ipykernel_17992/3522664596.py:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
      "/var/folders/8x/sy02cz7j5z3310hqp4hcxkgh0000gn/T/ipykernel_17992/3522664596.py:13: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
      "/var/folders/8x/sy02cz7j5z3310hqp4hcxkgh0000gn/T/ipykernel_17992/3522664596.py:14: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8x/sy02cz7j5z3310hqp4hcxkgh0000gn/T/ipykernel_17992/1919402437.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
      "/var/folders/8x/sy02cz7j5z3310hqp4hcxkgh0000gn/T/ipykernel_17992/1919402437.py:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
      "/var/folders/8x/sy02cz7j5z3310hqp4hcxkgh0000gn/T/ipykernel_17992/1919402437.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.ones(batch_size, dtype=np.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.609527\n",
      "Epoch 1, loss: 2.597418\n",
      "Epoch 2, loss: 2.584921\n",
      "Epoch 3, loss: 2.574316\n",
      "Epoch 4, loss: 2.562968\n",
      "Epoch 5, loss: 2.552624\n",
      "Epoch 6, loss: 2.543401\n",
      "Epoch 7, loss: 2.533715\n",
      "Epoch 8, loss: 2.523931\n",
      "Epoch 9, loss: 2.514965\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.081\n",
      "Epoch 0, loss: 2.506921\n",
      "Epoch 1, loss: 2.498456\n",
      "Epoch 2, loss: 2.490902\n",
      "Epoch 3, loss: 2.483121\n",
      "Epoch 4, loss: 2.476587\n",
      "Epoch 5, loss: 2.469424\n",
      "Epoch 6, loss: 2.463344\n",
      "Epoch 7, loss: 2.456666\n",
      "Epoch 8, loss: 2.450680\n",
      "Epoch 9, loss: 2.444057\n",
      "Epoch 10, loss: 2.438429\n",
      "Epoch 11, loss: 2.434408\n",
      "Epoch 12, loss: 2.427603\n",
      "Epoch 13, loss: 2.423400\n",
      "Epoch 14, loss: 2.417945\n",
      "Epoch 15, loss: 2.413439\n",
      "Epoch 16, loss: 2.409128\n",
      "Epoch 17, loss: 2.404512\n",
      "Epoch 18, loss: 2.399683\n",
      "Epoch 19, loss: 2.397412\n",
      "Epoch 20, loss: 2.392458\n",
      "Epoch 21, loss: 2.391070\n",
      "Epoch 22, loss: 2.386116\n",
      "Epoch 23, loss: 2.383185\n",
      "Epoch 24, loss: 2.380091\n",
      "Epoch 25, loss: 2.376628\n",
      "Epoch 26, loss: 2.373981\n",
      "Epoch 27, loss: 2.371100\n",
      "Epoch 28, loss: 2.368646\n",
      "Epoch 29, loss: 2.365458\n",
      "Epoch 30, loss: 2.363357\n",
      "Epoch 31, loss: 2.360728\n",
      "Epoch 32, loss: 2.358429\n",
      "Epoch 33, loss: 2.355761\n",
      "Epoch 34, loss: 2.353233\n",
      "Epoch 35, loss: 2.351461\n",
      "Epoch 36, loss: 2.349492\n",
      "Epoch 37, loss: 2.348074\n",
      "Epoch 38, loss: 2.345816\n",
      "Epoch 39, loss: 2.345044\n",
      "Epoch 40, loss: 2.343140\n",
      "Epoch 41, loss: 2.340236\n",
      "Epoch 42, loss: 2.339139\n",
      "Epoch 43, loss: 2.338175\n",
      "Epoch 44, loss: 2.336547\n",
      "Epoch 45, loss: 2.335135\n",
      "Epoch 46, loss: 2.333318\n",
      "Epoch 47, loss: 2.332436\n",
      "Epoch 48, loss: 2.331260\n",
      "Epoch 49, loss: 2.330119\n",
      "Epoch 50, loss: 2.329129\n",
      "Epoch 51, loss: 2.326975\n",
      "Epoch 52, loss: 2.326932\n",
      "Epoch 53, loss: 2.326076\n",
      "Epoch 54, loss: 2.326450\n",
      "Epoch 55, loss: 2.322677\n",
      "Epoch 56, loss: 2.322749\n",
      "Epoch 57, loss: 2.322428\n",
      "Epoch 58, loss: 2.320504\n",
      "Epoch 59, loss: 2.320902\n",
      "Epoch 60, loss: 2.319348\n",
      "Epoch 61, loss: 2.319079\n",
      "Epoch 62, loss: 2.318463\n",
      "Epoch 63, loss: 2.317557\n",
      "Epoch 64, loss: 2.317469\n",
      "Epoch 65, loss: 2.316948\n",
      "Epoch 66, loss: 2.316510\n",
      "Epoch 67, loss: 2.316551\n",
      "Epoch 68, loss: 2.315216\n",
      "Epoch 69, loss: 2.313455\n",
      "Epoch 70, loss: 2.313818\n",
      "Epoch 71, loss: 2.314366\n",
      "Epoch 72, loss: 2.312925\n",
      "Epoch 73, loss: 2.312478\n",
      "Epoch 74, loss: 2.312263\n",
      "Epoch 75, loss: 2.311877\n",
      "Epoch 76, loss: 2.311402\n",
      "Epoch 77, loss: 2.311589\n",
      "Epoch 78, loss: 2.310320\n",
      "Epoch 79, loss: 2.310512\n",
      "Epoch 80, loss: 2.310554\n",
      "Epoch 81, loss: 2.309462\n",
      "Epoch 82, loss: 2.309753\n",
      "Epoch 83, loss: 2.309362\n",
      "Epoch 84, loss: 2.308620\n",
      "Epoch 85, loss: 2.307967\n",
      "Epoch 86, loss: 2.307681\n",
      "Epoch 87, loss: 2.306599\n",
      "Epoch 88, loss: 2.307711\n",
      "Epoch 89, loss: 2.307228\n",
      "Epoch 90, loss: 2.306537\n",
      "Epoch 91, loss: 2.307758\n",
      "Epoch 92, loss: 2.306124\n",
      "Epoch 93, loss: 2.306075\n",
      "Epoch 94, loss: 2.307069\n",
      "Epoch 95, loss: 2.306568\n",
      "Epoch 96, loss: 2.306048\n",
      "Epoch 97, loss: 2.305043\n",
      "Epoch 98, loss: 2.305823\n",
      "Epoch 99, loss: 2.306092\n",
      "Accuracy after training for 100 epochs:  0.116\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.302396\n",
      "Epoch 1, loss: 2.302591\n",
      "Epoch 2, loss: 2.303109\n",
      "Epoch 3, loss: 2.303922\n",
      "Epoch 4, loss: 2.302140\n",
      "Epoch 5, loss: 2.301983\n",
      "Epoch 6, loss: 2.302246\n",
      "Epoch 7, loss: 2.302089\n",
      "Epoch 8, loss: 2.302306\n",
      "Epoch 9, loss: 2.302313\n",
      "Epoch 10, loss: 2.302272\n",
      "Epoch 11, loss: 2.301291\n",
      "Epoch 12, loss: 2.302850\n",
      "Epoch 13, loss: 2.301780\n",
      "Epoch 14, loss: 2.302055\n",
      "Epoch 15, loss: 2.301894\n",
      "Epoch 16, loss: 2.301629\n",
      "Epoch 17, loss: 2.302109\n",
      "Epoch 18, loss: 2.302277\n",
      "Epoch 19, loss: 2.302362\n",
      "Epoch 20, loss: 2.301332\n",
      "Epoch 21, loss: 2.301355\n",
      "Epoch 22, loss: 2.301184\n",
      "Epoch 23, loss: 2.300870\n",
      "Epoch 24, loss: 2.301133\n",
      "Epoch 25, loss: 2.302102\n",
      "Epoch 26, loss: 2.302120\n",
      "Epoch 27, loss: 2.300845\n",
      "Epoch 28, loss: 2.301672\n",
      "Epoch 29, loss: 2.300068\n",
      "Epoch 30, loss: 2.300072\n",
      "Epoch 31, loss: 2.302006\n",
      "Epoch 32, loss: 2.300437\n",
      "Epoch 33, loss: 2.301352\n",
      "Epoch 34, loss: 2.300620\n",
      "Epoch 35, loss: 2.300571\n",
      "Epoch 36, loss: 2.302552\n",
      "Epoch 37, loss: 2.300511\n",
      "Epoch 38, loss: 2.301957\n",
      "Epoch 39, loss: 2.301770\n",
      "Epoch 40, loss: 2.302371\n",
      "Epoch 41, loss: 2.300466\n",
      "Epoch 42, loss: 2.299428\n",
      "Epoch 43, loss: 2.301501\n",
      "Epoch 44, loss: 2.300442\n",
      "Epoch 45, loss: 2.301210\n",
      "Epoch 46, loss: 2.302306\n",
      "Epoch 47, loss: 2.299746\n",
      "Epoch 48, loss: 2.302537\n",
      "Epoch 49, loss: 2.300409\n",
      "Epoch 50, loss: 2.301365\n",
      "Epoch 51, loss: 2.300225\n",
      "Epoch 52, loss: 2.299484\n",
      "Epoch 53, loss: 2.300333\n",
      "Epoch 54, loss: 2.299897\n",
      "Epoch 55, loss: 2.300904\n",
      "Epoch 56, loss: 2.300994\n",
      "Epoch 57, loss: 2.298709\n",
      "Epoch 58, loss: 2.302334\n",
      "Epoch 59, loss: 2.298215\n",
      "Epoch 60, loss: 2.298322\n",
      "Epoch 61, loss: 2.299234\n",
      "Epoch 62, loss: 2.299945\n",
      "Epoch 63, loss: 2.299945\n",
      "Epoch 64, loss: 2.299098\n",
      "Epoch 65, loss: 2.299338\n",
      "Epoch 66, loss: 2.301172\n",
      "Epoch 67, loss: 2.300960\n",
      "Epoch 68, loss: 2.300002\n",
      "Epoch 69, loss: 2.300018\n",
      "Epoch 70, loss: 2.298134\n",
      "Epoch 71, loss: 2.301561\n",
      "Epoch 72, loss: 2.299251\n",
      "Epoch 73, loss: 2.299896\n",
      "Epoch 74, loss: 2.302076\n",
      "Epoch 75, loss: 2.297987\n",
      "Epoch 76, loss: 2.298138\n",
      "Epoch 77, loss: 2.303133\n",
      "Epoch 78, loss: 2.299309\n",
      "Epoch 79, loss: 2.299421\n",
      "Epoch 80, loss: 2.301119\n",
      "Epoch 81, loss: 2.300103\n",
      "Epoch 82, loss: 2.299751\n",
      "Epoch 83, loss: 2.300477\n",
      "Epoch 84, loss: 2.297720\n",
      "Epoch 85, loss: 2.299137\n",
      "Epoch 86, loss: 2.296731\n",
      "Epoch 87, loss: 2.297982\n",
      "Epoch 88, loss: 2.298471\n",
      "Epoch 89, loss: 2.296310\n",
      "Epoch 90, loss: 2.298104\n",
      "Epoch 91, loss: 2.298764\n",
      "Epoch 92, loss: 2.299945\n",
      "Epoch 93, loss: 2.298470\n",
      "Epoch 94, loss: 2.298322\n",
      "Epoch 95, loss: 2.297960\n",
      "Epoch 96, loss: 2.300925\n",
      "Epoch 97, loss: 2.299606\n",
      "Epoch 98, loss: 2.299873\n",
      "Epoch 99, loss: 2.299237\n",
      "Epoch 100, loss: 2.300949\n",
      "Epoch 101, loss: 2.298719\n",
      "Epoch 102, loss: 2.299622\n",
      "Epoch 103, loss: 2.300056\n",
      "Epoch 104, loss: 2.298794\n",
      "Epoch 105, loss: 2.300509\n",
      "Epoch 106, loss: 2.298741\n",
      "Epoch 107, loss: 2.297718\n",
      "Epoch 108, loss: 2.298618\n",
      "Epoch 109, loss: 2.299526\n",
      "Epoch 110, loss: 2.297355\n",
      "Epoch 111, loss: 2.294403\n",
      "Epoch 112, loss: 2.297422\n",
      "Epoch 113, loss: 2.298013\n",
      "Epoch 114, loss: 2.298142\n",
      "Epoch 115, loss: 2.297553\n",
      "Epoch 116, loss: 2.296021\n",
      "Epoch 117, loss: 2.297609\n",
      "Epoch 118, loss: 2.298615\n",
      "Epoch 119, loss: 2.295736\n",
      "Epoch 120, loss: 2.298335\n",
      "Epoch 121, loss: 2.297457\n",
      "Epoch 122, loss: 2.294550\n",
      "Epoch 123, loss: 2.296836\n",
      "Epoch 124, loss: 2.299267\n",
      "Epoch 125, loss: 2.297199\n",
      "Epoch 126, loss: 2.298450\n",
      "Epoch 127, loss: 2.299670\n",
      "Epoch 128, loss: 2.298224\n",
      "Epoch 129, loss: 2.294057\n",
      "Epoch 130, loss: 2.297435\n",
      "Epoch 131, loss: 2.301136\n",
      "Epoch 132, loss: 2.296820\n",
      "Epoch 133, loss: 2.296234\n",
      "Epoch 134, loss: 2.298256\n",
      "Epoch 135, loss: 2.298400\n",
      "Epoch 136, loss: 2.300287\n",
      "Epoch 137, loss: 2.297593\n",
      "Epoch 138, loss: 2.297295\n",
      "Epoch 139, loss: 2.298140\n",
      "Epoch 140, loss: 2.297009\n",
      "Epoch 141, loss: 2.297118\n",
      "Epoch 142, loss: 2.297147\n",
      "Epoch 143, loss: 2.297107\n",
      "Epoch 144, loss: 2.297061\n",
      "Epoch 145, loss: 2.298481\n",
      "Epoch 146, loss: 2.299329\n",
      "Epoch 147, loss: 2.298105\n",
      "Epoch 148, loss: 2.296053\n",
      "Epoch 149, loss: 2.295400\n",
      "Epoch 150, loss: 2.298309\n",
      "Epoch 151, loss: 2.295875\n",
      "Epoch 152, loss: 2.300883\n",
      "Epoch 153, loss: 2.297870\n",
      "Epoch 154, loss: 2.299588\n",
      "Epoch 155, loss: 2.297422\n",
      "Epoch 156, loss: 2.297209\n",
      "Epoch 157, loss: 2.295236\n",
      "Epoch 158, loss: 2.295855\n",
      "Epoch 159, loss: 2.295808\n",
      "Epoch 160, loss: 2.299894\n",
      "Epoch 161, loss: 2.295931\n",
      "Epoch 162, loss: 2.295864\n",
      "Epoch 163, loss: 2.293004\n",
      "Epoch 164, loss: 2.296276\n",
      "Epoch 165, loss: 2.294188\n",
      "Epoch 166, loss: 2.295700\n",
      "Epoch 167, loss: 2.301112\n",
      "Epoch 168, loss: 2.296553\n",
      "Epoch 169, loss: 2.299876\n",
      "Epoch 170, loss: 2.296698\n",
      "Epoch 171, loss: 2.297944\n",
      "Epoch 172, loss: 2.295941\n",
      "Epoch 173, loss: 2.294300\n",
      "Epoch 174, loss: 2.296478\n",
      "Epoch 175, loss: 2.299026\n",
      "Epoch 176, loss: 2.293642\n",
      "Epoch 177, loss: 2.298101\n",
      "Epoch 178, loss: 2.295965\n",
      "Epoch 179, loss: 2.296903\n",
      "Epoch 180, loss: 2.295655\n",
      "Epoch 181, loss: 2.293309\n",
      "Epoch 182, loss: 2.293270\n",
      "Epoch 183, loss: 2.296841\n",
      "Epoch 184, loss: 2.297551\n",
      "Epoch 185, loss: 2.295156\n",
      "Epoch 186, loss: 2.296175\n",
      "Epoch 187, loss: 2.295525\n",
      "Epoch 188, loss: 2.297127\n",
      "Epoch 189, loss: 2.295933\n",
      "Epoch 190, loss: 2.293610\n",
      "Epoch 191, loss: 2.294849\n",
      "Epoch 192, loss: 2.296045\n",
      "Epoch 193, loss: 2.293318\n",
      "Epoch 194, loss: 2.294456\n",
      "Epoch 195, loss: 2.297476\n",
      "Epoch 196, loss: 2.293761\n",
      "Epoch 197, loss: 2.297299\n",
      "Epoch 198, loss: 2.292842\n",
      "Epoch 199, loss: 2.298302\n",
      "Epoch 0, loss: 2.301678\n",
      "Epoch 1, loss: 2.302010\n",
      "Epoch 2, loss: 2.302194\n",
      "Epoch 3, loss: 2.302418\n",
      "Epoch 4, loss: 2.302306\n",
      "Epoch 5, loss: 2.302788\n",
      "Epoch 6, loss: 2.301699\n",
      "Epoch 7, loss: 2.302636\n",
      "Epoch 8, loss: 2.302321\n",
      "Epoch 9, loss: 2.302031\n",
      "Epoch 10, loss: 2.302549\n",
      "Epoch 11, loss: 2.302934\n",
      "Epoch 12, loss: 2.302337\n",
      "Epoch 13, loss: 2.302036\n",
      "Epoch 14, loss: 2.302377\n",
      "Epoch 15, loss: 2.301439\n",
      "Epoch 16, loss: 2.301386\n",
      "Epoch 17, loss: 2.301924\n",
      "Epoch 18, loss: 2.301981\n",
      "Epoch 19, loss: 2.302310\n",
      "Epoch 20, loss: 2.301821\n",
      "Epoch 21, loss: 2.302461\n",
      "Epoch 22, loss: 2.301529\n",
      "Epoch 23, loss: 2.302357\n",
      "Epoch 24, loss: 2.302374\n",
      "Epoch 25, loss: 2.301780\n",
      "Epoch 26, loss: 2.301195\n",
      "Epoch 27, loss: 2.301411\n",
      "Epoch 28, loss: 2.301430\n",
      "Epoch 29, loss: 2.301752\n",
      "Epoch 30, loss: 2.301309\n",
      "Epoch 31, loss: 2.302363\n",
      "Epoch 32, loss: 2.302526\n",
      "Epoch 33, loss: 2.301605\n",
      "Epoch 34, loss: 2.301319\n",
      "Epoch 35, loss: 2.301510\n",
      "Epoch 36, loss: 2.301212\n",
      "Epoch 37, loss: 2.301225\n",
      "Epoch 38, loss: 2.301162\n",
      "Epoch 39, loss: 2.301987\n",
      "Epoch 40, loss: 2.300404\n",
      "Epoch 41, loss: 2.299816\n",
      "Epoch 42, loss: 2.300829\n",
      "Epoch 43, loss: 2.301034\n",
      "Epoch 44, loss: 2.300531\n",
      "Epoch 45, loss: 2.301097\n",
      "Epoch 46, loss: 2.301502\n",
      "Epoch 47, loss: 2.302066\n",
      "Epoch 48, loss: 2.301913\n",
      "Epoch 49, loss: 2.301695\n",
      "Epoch 50, loss: 2.301982\n",
      "Epoch 51, loss: 2.301043\n",
      "Epoch 52, loss: 2.300500\n",
      "Epoch 53, loss: 2.301137\n",
      "Epoch 54, loss: 2.300410\n",
      "Epoch 55, loss: 2.298372\n",
      "Epoch 56, loss: 2.301613\n",
      "Epoch 57, loss: 2.299654\n",
      "Epoch 58, loss: 2.300828\n",
      "Epoch 59, loss: 2.300835\n",
      "Epoch 60, loss: 2.301254\n",
      "Epoch 61, loss: 2.300930\n",
      "Epoch 62, loss: 2.301069\n",
      "Epoch 63, loss: 2.301214\n",
      "Epoch 64, loss: 2.300022\n",
      "Epoch 65, loss: 2.300033\n",
      "Epoch 66, loss: 2.300496\n",
      "Epoch 67, loss: 2.299663\n",
      "Epoch 68, loss: 2.300296\n",
      "Epoch 69, loss: 2.300213\n",
      "Epoch 70, loss: 2.300553\n",
      "Epoch 71, loss: 2.300435\n",
      "Epoch 72, loss: 2.299232\n",
      "Epoch 73, loss: 2.300218\n",
      "Epoch 74, loss: 2.301647\n",
      "Epoch 75, loss: 2.299603\n",
      "Epoch 76, loss: 2.300300\n",
      "Epoch 77, loss: 2.299031\n",
      "Epoch 78, loss: 2.300514\n",
      "Epoch 79, loss: 2.300633\n",
      "Epoch 80, loss: 2.300126\n",
      "Epoch 81, loss: 2.299852\n",
      "Epoch 82, loss: 2.299828\n",
      "Epoch 83, loss: 2.300675\n",
      "Epoch 84, loss: 2.299508\n",
      "Epoch 85, loss: 2.300437\n",
      "Epoch 86, loss: 2.299096\n",
      "Epoch 87, loss: 2.297863\n",
      "Epoch 88, loss: 2.300214\n",
      "Epoch 89, loss: 2.300446\n",
      "Epoch 90, loss: 2.299327\n",
      "Epoch 91, loss: 2.299635\n",
      "Epoch 92, loss: 2.300396\n",
      "Epoch 93, loss: 2.299182\n",
      "Epoch 94, loss: 2.297970\n",
      "Epoch 95, loss: 2.297755\n",
      "Epoch 96, loss: 2.300181\n",
      "Epoch 97, loss: 2.298250\n",
      "Epoch 98, loss: 2.298219\n",
      "Epoch 99, loss: 2.299555\n",
      "Epoch 100, loss: 2.298287\n",
      "Epoch 101, loss: 2.300709\n",
      "Epoch 102, loss: 2.300209\n",
      "Epoch 103, loss: 2.298023\n",
      "Epoch 104, loss: 2.299911\n",
      "Epoch 105, loss: 2.300227\n",
      "Epoch 106, loss: 2.298468\n",
      "Epoch 107, loss: 2.300783\n",
      "Epoch 108, loss: 2.299093\n",
      "Epoch 109, loss: 2.297619\n",
      "Epoch 110, loss: 2.298134\n",
      "Epoch 111, loss: 2.298044\n",
      "Epoch 112, loss: 2.296103\n",
      "Epoch 113, loss: 2.297470\n",
      "Epoch 114, loss: 2.299306\n",
      "Epoch 115, loss: 2.298522\n",
      "Epoch 116, loss: 2.299074\n",
      "Epoch 117, loss: 2.298501\n",
      "Epoch 118, loss: 2.296346\n",
      "Epoch 119, loss: 2.299443\n",
      "Epoch 120, loss: 2.297603\n",
      "Epoch 121, loss: 2.300038\n",
      "Epoch 122, loss: 2.297806\n",
      "Epoch 123, loss: 2.298764\n",
      "Epoch 124, loss: 2.297571\n",
      "Epoch 125, loss: 2.298233\n",
      "Epoch 126, loss: 2.300102\n",
      "Epoch 127, loss: 2.299509\n",
      "Epoch 128, loss: 2.299385\n",
      "Epoch 129, loss: 2.299345\n",
      "Epoch 130, loss: 2.299343\n",
      "Epoch 131, loss: 2.298349\n",
      "Epoch 132, loss: 2.299213\n",
      "Epoch 133, loss: 2.297961\n",
      "Epoch 134, loss: 2.300691\n",
      "Epoch 135, loss: 2.299674\n",
      "Epoch 136, loss: 2.298857\n",
      "Epoch 137, loss: 2.297959\n",
      "Epoch 138, loss: 2.300797\n",
      "Epoch 139, loss: 2.298499\n",
      "Epoch 140, loss: 2.296690\n",
      "Epoch 141, loss: 2.298178\n",
      "Epoch 142, loss: 2.297117\n",
      "Epoch 143, loss: 2.297123\n",
      "Epoch 144, loss: 2.295871\n",
      "Epoch 145, loss: 2.296593\n",
      "Epoch 146, loss: 2.297647\n",
      "Epoch 147, loss: 2.299956\n",
      "Epoch 148, loss: 2.296199\n",
      "Epoch 149, loss: 2.297932\n",
      "Epoch 150, loss: 2.298400\n",
      "Epoch 151, loss: 2.299251\n",
      "Epoch 152, loss: 2.294243\n",
      "Epoch 153, loss: 2.299028\n",
      "Epoch 154, loss: 2.297226\n",
      "Epoch 155, loss: 2.297855\n",
      "Epoch 156, loss: 2.296750\n",
      "Epoch 157, loss: 2.296999\n",
      "Epoch 158, loss: 2.297930\n",
      "Epoch 159, loss: 2.297216\n",
      "Epoch 160, loss: 2.298841\n",
      "Epoch 161, loss: 2.296344\n",
      "Epoch 162, loss: 2.297826\n",
      "Epoch 163, loss: 2.297693\n",
      "Epoch 164, loss: 2.295576\n",
      "Epoch 165, loss: 2.298143\n",
      "Epoch 166, loss: 2.296182\n",
      "Epoch 167, loss: 2.296572\n",
      "Epoch 168, loss: 2.295831\n",
      "Epoch 169, loss: 2.297757\n",
      "Epoch 170, loss: 2.297793\n",
      "Epoch 171, loss: 2.295204\n",
      "Epoch 172, loss: 2.297588\n",
      "Epoch 173, loss: 2.298432\n",
      "Epoch 174, loss: 2.298771\n",
      "Epoch 175, loss: 2.297263\n",
      "Epoch 176, loss: 2.299399\n",
      "Epoch 177, loss: 2.297938\n",
      "Epoch 178, loss: 2.298054\n",
      "Epoch 179, loss: 2.297385\n",
      "Epoch 180, loss: 2.295071\n",
      "Epoch 181, loss: 2.295856\n",
      "Epoch 182, loss: 2.297897\n",
      "Epoch 183, loss: 2.296869\n",
      "Epoch 184, loss: 2.297781\n",
      "Epoch 185, loss: 2.300728\n",
      "Epoch 186, loss: 2.295870\n",
      "Epoch 187, loss: 2.293332\n",
      "Epoch 188, loss: 2.297981\n",
      "Epoch 189, loss: 2.297467\n",
      "Epoch 190, loss: 2.296047\n",
      "Epoch 191, loss: 2.297035\n",
      "Epoch 192, loss: 2.298039\n",
      "Epoch 193, loss: 2.296650\n",
      "Epoch 194, loss: 2.296088\n",
      "Epoch 195, loss: 2.296239\n",
      "Epoch 196, loss: 2.298571\n",
      "Epoch 197, loss: 2.293762\n",
      "Epoch 198, loss: 2.295976\n",
      "Epoch 199, loss: 2.296337\n",
      "Epoch 0, loss: 2.302886\n",
      "Epoch 1, loss: 2.301733\n",
      "Epoch 2, loss: 2.301813\n",
      "Epoch 3, loss: 2.302487\n",
      "Epoch 4, loss: 2.302086\n",
      "Epoch 5, loss: 2.303274\n",
      "Epoch 6, loss: 2.302919\n",
      "Epoch 7, loss: 2.302222\n",
      "Epoch 8, loss: 2.302654\n",
      "Epoch 9, loss: 2.302263\n",
      "Epoch 10, loss: 2.301629\n",
      "Epoch 11, loss: 2.301595\n",
      "Epoch 12, loss: 2.302229\n",
      "Epoch 13, loss: 2.302548\n",
      "Epoch 14, loss: 2.301716\n",
      "Epoch 15, loss: 2.301523\n",
      "Epoch 16, loss: 2.302008\n",
      "Epoch 17, loss: 2.301878\n",
      "Epoch 18, loss: 2.301986\n",
      "Epoch 19, loss: 2.302455\n",
      "Epoch 20, loss: 2.300795\n",
      "Epoch 21, loss: 2.302461\n",
      "Epoch 22, loss: 2.302383\n",
      "Epoch 23, loss: 2.301997\n",
      "Epoch 24, loss: 2.301375\n",
      "Epoch 25, loss: 2.302011\n",
      "Epoch 26, loss: 2.301241\n",
      "Epoch 27, loss: 2.300090\n",
      "Epoch 28, loss: 2.301806\n",
      "Epoch 29, loss: 2.301787\n",
      "Epoch 30, loss: 2.301559\n",
      "Epoch 31, loss: 2.302892\n",
      "Epoch 32, loss: 2.301015\n",
      "Epoch 33, loss: 2.301638\n",
      "Epoch 34, loss: 2.300986\n",
      "Epoch 35, loss: 2.301661\n",
      "Epoch 36, loss: 2.300820\n",
      "Epoch 37, loss: 2.300525\n",
      "Epoch 38, loss: 2.301478\n",
      "Epoch 39, loss: 2.301208\n",
      "Epoch 40, loss: 2.302550\n",
      "Epoch 41, loss: 2.301452\n",
      "Epoch 42, loss: 2.300759\n",
      "Epoch 43, loss: 2.301068\n",
      "Epoch 44, loss: 2.301762\n",
      "Epoch 45, loss: 2.300969\n",
      "Epoch 46, loss: 2.300958\n",
      "Epoch 47, loss: 2.300565\n",
      "Epoch 48, loss: 2.302424\n",
      "Epoch 49, loss: 2.300647\n",
      "Epoch 50, loss: 2.300221\n",
      "Epoch 51, loss: 2.300438\n",
      "Epoch 52, loss: 2.300238\n",
      "Epoch 53, loss: 2.301001\n",
      "Epoch 54, loss: 2.300683\n",
      "Epoch 55, loss: 2.301345\n",
      "Epoch 56, loss: 2.300545\n",
      "Epoch 57, loss: 2.300261\n",
      "Epoch 58, loss: 2.300628\n",
      "Epoch 59, loss: 2.299847\n",
      "Epoch 60, loss: 2.300831\n",
      "Epoch 61, loss: 2.299684\n",
      "Epoch 62, loss: 2.301590\n",
      "Epoch 63, loss: 2.300982\n",
      "Epoch 64, loss: 2.301401\n",
      "Epoch 65, loss: 2.299215\n",
      "Epoch 66, loss: 2.301416\n",
      "Epoch 67, loss: 2.300071\n",
      "Epoch 68, loss: 2.299802\n",
      "Epoch 69, loss: 2.299931\n",
      "Epoch 70, loss: 2.298963\n",
      "Epoch 71, loss: 2.300091\n",
      "Epoch 72, loss: 2.300756\n",
      "Epoch 73, loss: 2.299834\n",
      "Epoch 74, loss: 2.301062\n",
      "Epoch 75, loss: 2.299787\n",
      "Epoch 76, loss: 2.301581\n",
      "Epoch 77, loss: 2.300796\n",
      "Epoch 78, loss: 2.300700\n",
      "Epoch 79, loss: 2.300925\n",
      "Epoch 80, loss: 2.300695\n",
      "Epoch 81, loss: 2.300056\n",
      "Epoch 82, loss: 2.300342\n",
      "Epoch 83, loss: 2.300668\n",
      "Epoch 84, loss: 2.299682\n",
      "Epoch 85, loss: 2.300569\n",
      "Epoch 86, loss: 2.300581\n",
      "Epoch 87, loss: 2.299925\n",
      "Epoch 88, loss: 2.299201\n",
      "Epoch 89, loss: 2.298517\n",
      "Epoch 90, loss: 2.299452\n",
      "Epoch 91, loss: 2.298985\n",
      "Epoch 92, loss: 2.298782\n",
      "Epoch 93, loss: 2.300369\n",
      "Epoch 94, loss: 2.297751\n",
      "Epoch 95, loss: 2.299249\n",
      "Epoch 96, loss: 2.301072\n",
      "Epoch 97, loss: 2.300506\n",
      "Epoch 98, loss: 2.298068\n",
      "Epoch 99, loss: 2.298210\n",
      "Epoch 100, loss: 2.298827\n",
      "Epoch 101, loss: 2.299764\n",
      "Epoch 102, loss: 2.298356\n",
      "Epoch 103, loss: 2.300939\n",
      "Epoch 104, loss: 2.298698\n",
      "Epoch 105, loss: 2.300556\n",
      "Epoch 106, loss: 2.298488\n",
      "Epoch 107, loss: 2.301215\n",
      "Epoch 108, loss: 2.299919\n",
      "Epoch 109, loss: 2.299613\n",
      "Epoch 110, loss: 2.300817\n",
      "Epoch 111, loss: 2.299043\n",
      "Epoch 112, loss: 2.300408\n",
      "Epoch 113, loss: 2.298480\n",
      "Epoch 114, loss: 2.300250\n",
      "Epoch 115, loss: 2.299090\n",
      "Epoch 116, loss: 2.299308\n",
      "Epoch 117, loss: 2.297466\n",
      "Epoch 118, loss: 2.298404\n",
      "Epoch 119, loss: 2.297565\n",
      "Epoch 120, loss: 2.299076\n",
      "Epoch 121, loss: 2.298476\n",
      "Epoch 122, loss: 2.298428\n",
      "Epoch 123, loss: 2.298407\n",
      "Epoch 124, loss: 2.298465\n",
      "Epoch 125, loss: 2.298431\n",
      "Epoch 126, loss: 2.298462\n",
      "Epoch 127, loss: 2.301528\n",
      "Epoch 128, loss: 2.295148\n",
      "Epoch 129, loss: 2.298148\n",
      "Epoch 130, loss: 2.298081\n",
      "Epoch 131, loss: 2.301148\n",
      "Epoch 132, loss: 2.299647\n",
      "Epoch 133, loss: 2.298997\n",
      "Epoch 134, loss: 2.298549\n",
      "Epoch 135, loss: 2.298588\n",
      "Epoch 136, loss: 2.300588\n",
      "Epoch 137, loss: 2.297356\n",
      "Epoch 138, loss: 2.296612\n",
      "Epoch 139, loss: 2.299128\n",
      "Epoch 140, loss: 2.298343\n",
      "Epoch 141, loss: 2.296457\n",
      "Epoch 142, loss: 2.299662\n",
      "Epoch 143, loss: 2.296743\n",
      "Epoch 144, loss: 2.296667\n",
      "Epoch 145, loss: 2.298955\n",
      "Epoch 146, loss: 2.295550\n",
      "Epoch 147, loss: 2.296194\n",
      "Epoch 148, loss: 2.297408\n",
      "Epoch 149, loss: 2.297764\n",
      "Epoch 150, loss: 2.297056\n",
      "Epoch 151, loss: 2.295997\n",
      "Epoch 152, loss: 2.296152\n",
      "Epoch 153, loss: 2.296728\n",
      "Epoch 154, loss: 2.301347\n",
      "Epoch 155, loss: 2.295819\n",
      "Epoch 156, loss: 2.297604\n",
      "Epoch 157, loss: 2.297907\n",
      "Epoch 158, loss: 2.297104\n",
      "Epoch 159, loss: 2.299164\n",
      "Epoch 160, loss: 2.296585\n",
      "Epoch 161, loss: 2.297502\n",
      "Epoch 162, loss: 2.295938\n",
      "Epoch 163, loss: 2.299654\n",
      "Epoch 164, loss: 2.298213\n",
      "Epoch 165, loss: 2.298075\n",
      "Epoch 166, loss: 2.297169\n",
      "Epoch 167, loss: 2.295311\n",
      "Epoch 168, loss: 2.297773\n",
      "Epoch 169, loss: 2.295354\n",
      "Epoch 170, loss: 2.298934\n",
      "Epoch 171, loss: 2.296930\n",
      "Epoch 172, loss: 2.297842\n",
      "Epoch 173, loss: 2.294103\n",
      "Epoch 174, loss: 2.300077\n",
      "Epoch 175, loss: 2.294639\n",
      "Epoch 176, loss: 2.298824\n",
      "Epoch 177, loss: 2.296603\n",
      "Epoch 178, loss: 2.294035\n",
      "Epoch 179, loss: 2.297854\n",
      "Epoch 180, loss: 2.297040\n",
      "Epoch 181, loss: 2.296413\n",
      "Epoch 182, loss: 2.297780\n",
      "Epoch 183, loss: 2.297351\n",
      "Epoch 184, loss: 2.294166\n",
      "Epoch 185, loss: 2.293031\n",
      "Epoch 186, loss: 2.294632\n",
      "Epoch 187, loss: 2.295625\n",
      "Epoch 188, loss: 2.297019\n",
      "Epoch 189, loss: 2.297117\n",
      "Epoch 190, loss: 2.295453\n",
      "Epoch 191, loss: 2.296414\n",
      "Epoch 192, loss: 2.297528\n",
      "Epoch 193, loss: 2.297427\n",
      "Epoch 194, loss: 2.298132\n",
      "Epoch 195, loss: 2.295424\n",
      "Epoch 196, loss: 2.296986\n",
      "Epoch 197, loss: 2.297543\n",
      "Epoch 198, loss: 2.296184\n",
      "Epoch 199, loss: 2.292907\n",
      "Epoch 0, loss: 2.302610\n",
      "Epoch 1, loss: 2.302848\n",
      "Epoch 2, loss: 2.303067\n",
      "Epoch 3, loss: 2.302763\n",
      "Epoch 4, loss: 2.303226\n",
      "Epoch 5, loss: 2.302737\n",
      "Epoch 6, loss: 2.302428\n",
      "Epoch 7, loss: 2.303494\n",
      "Epoch 8, loss: 2.303195\n",
      "Epoch 9, loss: 2.302919\n",
      "Epoch 10, loss: 2.302369\n",
      "Epoch 11, loss: 2.303693\n",
      "Epoch 12, loss: 2.302220\n",
      "Epoch 13, loss: 2.302815\n",
      "Epoch 14, loss: 2.302727\n",
      "Epoch 15, loss: 2.301190\n",
      "Epoch 16, loss: 2.303039\n",
      "Epoch 17, loss: 2.302727\n",
      "Epoch 18, loss: 2.302866\n",
      "Epoch 19, loss: 2.302599\n",
      "Epoch 20, loss: 2.301906\n",
      "Epoch 21, loss: 2.302961\n",
      "Epoch 22, loss: 2.302599\n",
      "Epoch 23, loss: 2.303442\n",
      "Epoch 24, loss: 2.303213\n",
      "Epoch 25, loss: 2.302773\n",
      "Epoch 26, loss: 2.302568\n",
      "Epoch 27, loss: 2.303494\n",
      "Epoch 28, loss: 2.303020\n",
      "Epoch 29, loss: 2.303109\n",
      "Epoch 30, loss: 2.303269\n",
      "Epoch 31, loss: 2.303391\n",
      "Epoch 32, loss: 2.303523\n",
      "Epoch 33, loss: 2.302491\n",
      "Epoch 34, loss: 2.303505\n",
      "Epoch 35, loss: 2.302848\n",
      "Epoch 36, loss: 2.302540\n",
      "Epoch 37, loss: 2.303283\n",
      "Epoch 38, loss: 2.303695\n",
      "Epoch 39, loss: 2.302785\n",
      "Epoch 40, loss: 2.302462\n",
      "Epoch 41, loss: 2.302545\n",
      "Epoch 42, loss: 2.302527\n",
      "Epoch 43, loss: 2.302555\n",
      "Epoch 44, loss: 2.303350\n",
      "Epoch 45, loss: 2.302816\n",
      "Epoch 46, loss: 2.302446\n",
      "Epoch 47, loss: 2.302818\n",
      "Epoch 48, loss: 2.302993\n",
      "Epoch 49, loss: 2.302695\n",
      "Epoch 50, loss: 2.303079\n",
      "Epoch 51, loss: 2.302751\n",
      "Epoch 52, loss: 2.302101\n",
      "Epoch 53, loss: 2.302773\n",
      "Epoch 54, loss: 2.303187\n",
      "Epoch 55, loss: 2.301722\n",
      "Epoch 56, loss: 2.302001\n",
      "Epoch 57, loss: 2.302222\n",
      "Epoch 58, loss: 2.303045\n",
      "Epoch 59, loss: 2.302810\n",
      "Epoch 60, loss: 2.303163\n",
      "Epoch 61, loss: 2.303429\n",
      "Epoch 62, loss: 2.303107\n",
      "Epoch 63, loss: 2.302645\n",
      "Epoch 64, loss: 2.303442\n",
      "Epoch 65, loss: 2.303279\n",
      "Epoch 66, loss: 2.302484\n",
      "Epoch 67, loss: 2.301876\n",
      "Epoch 68, loss: 2.302234\n",
      "Epoch 69, loss: 2.303178\n",
      "Epoch 70, loss: 2.302649\n",
      "Epoch 71, loss: 2.302788\n",
      "Epoch 72, loss: 2.303439\n",
      "Epoch 73, loss: 2.302403\n",
      "Epoch 74, loss: 2.303462\n",
      "Epoch 75, loss: 2.303097\n",
      "Epoch 76, loss: 2.302064\n",
      "Epoch 77, loss: 2.303600\n",
      "Epoch 78, loss: 2.301970\n",
      "Epoch 79, loss: 2.303333\n",
      "Epoch 80, loss: 2.302018\n",
      "Epoch 81, loss: 2.302898\n",
      "Epoch 82, loss: 2.302855\n",
      "Epoch 83, loss: 2.302897\n",
      "Epoch 84, loss: 2.302239\n",
      "Epoch 85, loss: 2.302502\n",
      "Epoch 86, loss: 2.303828\n",
      "Epoch 87, loss: 2.303021\n",
      "Epoch 88, loss: 2.302885\n",
      "Epoch 89, loss: 2.303098\n",
      "Epoch 90, loss: 2.303101\n",
      "Epoch 91, loss: 2.302684\n",
      "Epoch 92, loss: 2.302574\n",
      "Epoch 93, loss: 2.302117\n",
      "Epoch 94, loss: 2.303262\n",
      "Epoch 95, loss: 2.302026\n",
      "Epoch 96, loss: 2.301437\n",
      "Epoch 97, loss: 2.301885\n",
      "Epoch 98, loss: 2.302331\n",
      "Epoch 99, loss: 2.303222\n",
      "Epoch 100, loss: 2.301666\n",
      "Epoch 101, loss: 2.302299\n",
      "Epoch 102, loss: 2.301731\n",
      "Epoch 103, loss: 2.302799\n",
      "Epoch 104, loss: 2.303264\n",
      "Epoch 105, loss: 2.301676\n",
      "Epoch 106, loss: 2.302082\n",
      "Epoch 107, loss: 2.303025\n",
      "Epoch 108, loss: 2.302328\n",
      "Epoch 109, loss: 2.302016\n",
      "Epoch 110, loss: 2.302856\n",
      "Epoch 111, loss: 2.302711\n",
      "Epoch 112, loss: 2.301956\n",
      "Epoch 113, loss: 2.303247\n",
      "Epoch 114, loss: 2.302721\n",
      "Epoch 115, loss: 2.301665\n",
      "Epoch 116, loss: 2.302708\n",
      "Epoch 117, loss: 2.302582\n",
      "Epoch 118, loss: 2.303729\n",
      "Epoch 119, loss: 2.301488\n",
      "Epoch 120, loss: 2.303244\n",
      "Epoch 121, loss: 2.301469\n",
      "Epoch 122, loss: 2.302754\n",
      "Epoch 123, loss: 2.301696\n",
      "Epoch 124, loss: 2.302871\n",
      "Epoch 125, loss: 2.302377\n",
      "Epoch 126, loss: 2.302591\n",
      "Epoch 127, loss: 2.302250\n",
      "Epoch 128, loss: 2.302679\n",
      "Epoch 129, loss: 2.302161\n",
      "Epoch 130, loss: 2.302530\n",
      "Epoch 131, loss: 2.303389\n",
      "Epoch 132, loss: 2.302390\n",
      "Epoch 133, loss: 2.302147\n",
      "Epoch 134, loss: 2.302700\n",
      "Epoch 135, loss: 2.302270\n",
      "Epoch 136, loss: 2.301572\n",
      "Epoch 137, loss: 2.302663\n",
      "Epoch 138, loss: 2.303450\n",
      "Epoch 139, loss: 2.301512\n",
      "Epoch 140, loss: 2.301673\n",
      "Epoch 141, loss: 2.302369\n",
      "Epoch 142, loss: 2.302507\n",
      "Epoch 143, loss: 2.302364\n",
      "Epoch 144, loss: 2.302326\n",
      "Epoch 145, loss: 2.302423\n",
      "Epoch 146, loss: 2.301824\n",
      "Epoch 147, loss: 2.302620\n",
      "Epoch 148, loss: 2.303563\n",
      "Epoch 149, loss: 2.301303\n",
      "Epoch 150, loss: 2.301241\n",
      "Epoch 151, loss: 2.301261\n",
      "Epoch 152, loss: 2.301774\n",
      "Epoch 153, loss: 2.302450\n",
      "Epoch 154, loss: 2.301417\n",
      "Epoch 155, loss: 2.302643\n",
      "Epoch 156, loss: 2.302868\n",
      "Epoch 157, loss: 2.302996\n",
      "Epoch 158, loss: 2.302379\n",
      "Epoch 159, loss: 2.301396\n",
      "Epoch 160, loss: 2.301778\n",
      "Epoch 161, loss: 2.302259\n",
      "Epoch 162, loss: 2.301196\n",
      "Epoch 163, loss: 2.302019\n",
      "Epoch 164, loss: 2.302633\n",
      "Epoch 165, loss: 2.302205\n",
      "Epoch 166, loss: 2.301574\n",
      "Epoch 167, loss: 2.302285\n",
      "Epoch 168, loss: 2.302147\n",
      "Epoch 169, loss: 2.301332\n",
      "Epoch 170, loss: 2.301518\n",
      "Epoch 171, loss: 2.303164\n",
      "Epoch 172, loss: 2.303004\n",
      "Epoch 173, loss: 2.302269\n",
      "Epoch 174, loss: 2.302411\n",
      "Epoch 175, loss: 2.302827\n",
      "Epoch 176, loss: 2.302197\n",
      "Epoch 177, loss: 2.303016\n",
      "Epoch 178, loss: 2.302416\n",
      "Epoch 179, loss: 2.302251\n",
      "Epoch 180, loss: 2.302065\n",
      "Epoch 181, loss: 2.302383\n",
      "Epoch 182, loss: 2.301783\n",
      "Epoch 183, loss: 2.301618\n",
      "Epoch 184, loss: 2.301027\n",
      "Epoch 185, loss: 2.303037\n",
      "Epoch 186, loss: 2.302275\n",
      "Epoch 187, loss: 2.301602\n",
      "Epoch 188, loss: 2.302692\n",
      "Epoch 189, loss: 2.302063\n",
      "Epoch 190, loss: 2.303115\n",
      "Epoch 191, loss: 2.300854\n",
      "Epoch 192, loss: 2.302170\n",
      "Epoch 193, loss: 2.302307\n",
      "Epoch 194, loss: 2.303046\n",
      "Epoch 195, loss: 2.301778\n",
      "Epoch 196, loss: 2.301846\n",
      "Epoch 197, loss: 2.302696\n",
      "Epoch 198, loss: 2.304723\n",
      "Epoch 199, loss: 2.301802\n",
      "Epoch 0, loss: 2.302419\n",
      "Epoch 1, loss: 2.302616\n",
      "Epoch 2, loss: 2.302580\n",
      "Epoch 3, loss: 2.302315\n",
      "Epoch 4, loss: 2.302818\n",
      "Epoch 5, loss: 2.303068\n",
      "Epoch 6, loss: 2.303101\n",
      "Epoch 7, loss: 2.301845\n",
      "Epoch 8, loss: 2.302182\n",
      "Epoch 9, loss: 2.302582\n",
      "Epoch 10, loss: 2.302932\n",
      "Epoch 11, loss: 2.301925\n",
      "Epoch 12, loss: 2.301131\n",
      "Epoch 13, loss: 2.301939\n",
      "Epoch 14, loss: 2.302509\n",
      "Epoch 15, loss: 2.301876\n",
      "Epoch 16, loss: 2.302447\n",
      "Epoch 17, loss: 2.301922\n",
      "Epoch 18, loss: 2.302534\n",
      "Epoch 19, loss: 2.302781\n",
      "Epoch 20, loss: 2.302387\n",
      "Epoch 21, loss: 2.301876\n",
      "Epoch 22, loss: 2.303050\n",
      "Epoch 23, loss: 2.303339\n",
      "Epoch 24, loss: 2.302083\n",
      "Epoch 25, loss: 2.302533\n",
      "Epoch 26, loss: 2.304183\n",
      "Epoch 27, loss: 2.302955\n",
      "Epoch 28, loss: 2.303595\n",
      "Epoch 29, loss: 2.300987\n",
      "Epoch 30, loss: 2.302189\n",
      "Epoch 31, loss: 2.303184\n",
      "Epoch 32, loss: 2.302482\n",
      "Epoch 33, loss: 2.301956\n",
      "Epoch 34, loss: 2.302422\n",
      "Epoch 35, loss: 2.302836\n",
      "Epoch 36, loss: 2.302217\n",
      "Epoch 37, loss: 2.304033\n",
      "Epoch 38, loss: 2.302932\n",
      "Epoch 39, loss: 2.302725\n",
      "Epoch 40, loss: 2.302383\n",
      "Epoch 41, loss: 2.301193\n",
      "Epoch 42, loss: 2.303216\n",
      "Epoch 43, loss: 2.302193\n",
      "Epoch 44, loss: 2.302561\n",
      "Epoch 45, loss: 2.301619\n",
      "Epoch 46, loss: 2.302143\n",
      "Epoch 47, loss: 2.302676\n",
      "Epoch 48, loss: 2.301745\n",
      "Epoch 49, loss: 2.302300\n",
      "Epoch 50, loss: 2.302707\n",
      "Epoch 51, loss: 2.303406\n",
      "Epoch 52, loss: 2.302202\n",
      "Epoch 53, loss: 2.302195\n",
      "Epoch 54, loss: 2.302106\n",
      "Epoch 55, loss: 2.301073\n",
      "Epoch 56, loss: 2.302739\n",
      "Epoch 57, loss: 2.302003\n",
      "Epoch 58, loss: 2.302046\n",
      "Epoch 59, loss: 2.302780\n",
      "Epoch 60, loss: 2.303062\n",
      "Epoch 61, loss: 2.302887\n",
      "Epoch 62, loss: 2.302310\n",
      "Epoch 63, loss: 2.301418\n",
      "Epoch 64, loss: 2.302393\n",
      "Epoch 65, loss: 2.302739\n",
      "Epoch 66, loss: 2.303211\n",
      "Epoch 67, loss: 2.301755\n",
      "Epoch 68, loss: 2.302825\n",
      "Epoch 69, loss: 2.302117\n",
      "Epoch 70, loss: 2.301445\n",
      "Epoch 71, loss: 2.301853\n",
      "Epoch 72, loss: 2.301596\n",
      "Epoch 73, loss: 2.303462\n",
      "Epoch 74, loss: 2.301664\n",
      "Epoch 75, loss: 2.301655\n",
      "Epoch 76, loss: 2.300958\n",
      "Epoch 77, loss: 2.302702\n",
      "Epoch 78, loss: 2.303134\n",
      "Epoch 79, loss: 2.301398\n",
      "Epoch 80, loss: 2.302605\n",
      "Epoch 81, loss: 2.302389\n",
      "Epoch 82, loss: 2.302486\n",
      "Epoch 83, loss: 2.301759\n",
      "Epoch 84, loss: 2.302867\n",
      "Epoch 85, loss: 2.303435\n",
      "Epoch 86, loss: 2.302566\n",
      "Epoch 87, loss: 2.302975\n",
      "Epoch 88, loss: 2.303019\n",
      "Epoch 89, loss: 2.302385\n",
      "Epoch 90, loss: 2.301577\n",
      "Epoch 91, loss: 2.302505\n",
      "Epoch 92, loss: 2.302438\n",
      "Epoch 93, loss: 2.302838\n",
      "Epoch 94, loss: 2.302242\n",
      "Epoch 95, loss: 2.301310\n",
      "Epoch 96, loss: 2.301434\n",
      "Epoch 97, loss: 2.301698\n",
      "Epoch 98, loss: 2.301206\n",
      "Epoch 99, loss: 2.303039\n",
      "Epoch 100, loss: 2.302466\n",
      "Epoch 101, loss: 2.302215\n",
      "Epoch 102, loss: 2.301808\n",
      "Epoch 103, loss: 2.301764\n",
      "Epoch 104, loss: 2.302855\n",
      "Epoch 105, loss: 2.303096\n",
      "Epoch 106, loss: 2.301840\n",
      "Epoch 107, loss: 2.302738\n",
      "Epoch 108, loss: 2.302695\n",
      "Epoch 109, loss: 2.302006\n",
      "Epoch 110, loss: 2.300963\n",
      "Epoch 111, loss: 2.302340\n",
      "Epoch 112, loss: 2.302138\n",
      "Epoch 113, loss: 2.301519\n",
      "Epoch 114, loss: 2.301201\n",
      "Epoch 115, loss: 2.302284\n",
      "Epoch 116, loss: 2.302169\n",
      "Epoch 117, loss: 2.303487\n",
      "Epoch 118, loss: 2.302708\n",
      "Epoch 119, loss: 2.302040\n",
      "Epoch 120, loss: 2.301343\n",
      "Epoch 121, loss: 2.301424\n",
      "Epoch 122, loss: 2.301774\n",
      "Epoch 123, loss: 2.301751\n",
      "Epoch 124, loss: 2.302800\n",
      "Epoch 125, loss: 2.302855\n",
      "Epoch 126, loss: 2.301995\n",
      "Epoch 127, loss: 2.303150\n",
      "Epoch 128, loss: 2.302977\n",
      "Epoch 129, loss: 2.301030\n",
      "Epoch 130, loss: 2.302216\n",
      "Epoch 131, loss: 2.303196\n",
      "Epoch 132, loss: 2.302068\n",
      "Epoch 133, loss: 2.303610\n",
      "Epoch 134, loss: 2.302562\n",
      "Epoch 135, loss: 2.301840\n",
      "Epoch 136, loss: 2.300433\n",
      "Epoch 137, loss: 2.301712\n",
      "Epoch 138, loss: 2.301706\n",
      "Epoch 139, loss: 2.302516\n",
      "Epoch 140, loss: 2.302023\n",
      "Epoch 141, loss: 2.302322\n",
      "Epoch 142, loss: 2.302039\n",
      "Epoch 143, loss: 2.302312\n",
      "Epoch 144, loss: 2.301667\n",
      "Epoch 145, loss: 2.302644\n",
      "Epoch 146, loss: 2.301473\n",
      "Epoch 147, loss: 2.301668\n",
      "Epoch 148, loss: 2.303315\n",
      "Epoch 149, loss: 2.302486\n",
      "Epoch 150, loss: 2.303369\n",
      "Epoch 151, loss: 2.302353\n",
      "Epoch 152, loss: 2.301859\n",
      "Epoch 153, loss: 2.303523\n",
      "Epoch 154, loss: 2.302443\n",
      "Epoch 155, loss: 2.301538\n",
      "Epoch 156, loss: 2.300452\n",
      "Epoch 157, loss: 2.301834\n",
      "Epoch 158, loss: 2.302489\n",
      "Epoch 159, loss: 2.302543\n",
      "Epoch 160, loss: 2.303252\n",
      "Epoch 161, loss: 2.302600\n",
      "Epoch 162, loss: 2.303048\n",
      "Epoch 163, loss: 2.300909\n",
      "Epoch 164, loss: 2.300998\n",
      "Epoch 165, loss: 2.301628\n",
      "Epoch 166, loss: 2.302288\n",
      "Epoch 167, loss: 2.300838\n",
      "Epoch 168, loss: 2.303823\n",
      "Epoch 169, loss: 2.301663\n",
      "Epoch 170, loss: 2.301753\n",
      "Epoch 171, loss: 2.302392\n",
      "Epoch 172, loss: 2.301270\n",
      "Epoch 173, loss: 2.301597\n",
      "Epoch 174, loss: 2.304116\n",
      "Epoch 175, loss: 2.302063\n",
      "Epoch 176, loss: 2.302435\n",
      "Epoch 177, loss: 2.302448\n",
      "Epoch 178, loss: 2.302117\n",
      "Epoch 179, loss: 2.302783\n",
      "Epoch 180, loss: 2.302278\n",
      "Epoch 181, loss: 2.302564\n",
      "Epoch 182, loss: 2.302437\n",
      "Epoch 183, loss: 2.302790\n",
      "Epoch 184, loss: 2.303239\n",
      "Epoch 185, loss: 2.301803\n",
      "Epoch 186, loss: 2.302501\n",
      "Epoch 187, loss: 2.300893\n",
      "Epoch 188, loss: 2.301676\n",
      "Epoch 189, loss: 2.303296\n",
      "Epoch 190, loss: 2.304261\n",
      "Epoch 191, loss: 2.301285\n",
      "Epoch 192, loss: 2.302363\n",
      "Epoch 193, loss: 2.301373\n",
      "Epoch 194, loss: 2.301423\n",
      "Epoch 195, loss: 2.302111\n",
      "Epoch 196, loss: 2.302082\n",
      "Epoch 197, loss: 2.302482\n",
      "Epoch 198, loss: 2.302277\n",
      "Epoch 199, loss: 2.302426\n",
      "Epoch 0, loss: 2.302366\n",
      "Epoch 1, loss: 2.303020\n",
      "Epoch 2, loss: 2.302305\n",
      "Epoch 3, loss: 2.303264\n",
      "Epoch 4, loss: 2.304288\n",
      "Epoch 5, loss: 2.302048\n",
      "Epoch 6, loss: 2.301861\n",
      "Epoch 7, loss: 2.302657\n",
      "Epoch 8, loss: 2.303014\n",
      "Epoch 9, loss: 2.303157\n",
      "Epoch 10, loss: 2.301841\n",
      "Epoch 11, loss: 2.303969\n",
      "Epoch 12, loss: 2.302846\n",
      "Epoch 13, loss: 2.303310\n",
      "Epoch 14, loss: 2.302907\n",
      "Epoch 15, loss: 2.302901\n",
      "Epoch 16, loss: 2.303058\n",
      "Epoch 17, loss: 2.302845\n",
      "Epoch 18, loss: 2.302110\n",
      "Epoch 19, loss: 2.300680\n",
      "Epoch 20, loss: 2.302151\n",
      "Epoch 21, loss: 2.302252\n",
      "Epoch 22, loss: 2.303300\n",
      "Epoch 23, loss: 2.301530\n",
      "Epoch 24, loss: 2.302937\n",
      "Epoch 25, loss: 2.302053\n",
      "Epoch 26, loss: 2.301747\n",
      "Epoch 27, loss: 2.302919\n",
      "Epoch 28, loss: 2.302332\n",
      "Epoch 29, loss: 2.302801\n",
      "Epoch 30, loss: 2.302076\n",
      "Epoch 31, loss: 2.303544\n",
      "Epoch 32, loss: 2.303431\n",
      "Epoch 33, loss: 2.302757\n",
      "Epoch 34, loss: 2.303871\n",
      "Epoch 35, loss: 2.302517\n",
      "Epoch 36, loss: 2.302923\n",
      "Epoch 37, loss: 2.302274\n",
      "Epoch 38, loss: 2.302591\n",
      "Epoch 39, loss: 2.303304\n",
      "Epoch 40, loss: 2.302102\n",
      "Epoch 41, loss: 2.303203\n",
      "Epoch 42, loss: 2.302484\n",
      "Epoch 43, loss: 2.302757\n",
      "Epoch 44, loss: 2.302609\n",
      "Epoch 45, loss: 2.302133\n",
      "Epoch 46, loss: 2.302644\n",
      "Epoch 47, loss: 2.301366\n",
      "Epoch 48, loss: 2.301270\n",
      "Epoch 49, loss: 2.301831\n",
      "Epoch 50, loss: 2.303419\n",
      "Epoch 51, loss: 2.301132\n",
      "Epoch 52, loss: 2.302541\n",
      "Epoch 53, loss: 2.301005\n",
      "Epoch 54, loss: 2.302987\n",
      "Epoch 55, loss: 2.303227\n",
      "Epoch 56, loss: 2.303074\n",
      "Epoch 57, loss: 2.302703\n",
      "Epoch 58, loss: 2.301103\n",
      "Epoch 59, loss: 2.302307\n",
      "Epoch 60, loss: 2.302576\n",
      "Epoch 61, loss: 2.302136\n",
      "Epoch 62, loss: 2.302062\n",
      "Epoch 63, loss: 2.302052\n",
      "Epoch 64, loss: 2.301991\n",
      "Epoch 65, loss: 2.302300\n",
      "Epoch 66, loss: 2.301660\n",
      "Epoch 67, loss: 2.301767\n",
      "Epoch 68, loss: 2.302974\n",
      "Epoch 69, loss: 2.302403\n",
      "Epoch 70, loss: 2.301998\n",
      "Epoch 71, loss: 2.302167\n",
      "Epoch 72, loss: 2.302586\n",
      "Epoch 73, loss: 2.302687\n",
      "Epoch 74, loss: 2.301490\n",
      "Epoch 75, loss: 2.302886\n",
      "Epoch 76, loss: 2.301303\n",
      "Epoch 77, loss: 2.301874\n",
      "Epoch 78, loss: 2.301518\n",
      "Epoch 79, loss: 2.301668\n",
      "Epoch 80, loss: 2.301790\n",
      "Epoch 81, loss: 2.302247\n",
      "Epoch 82, loss: 2.302415\n",
      "Epoch 83, loss: 2.302644\n",
      "Epoch 84, loss: 2.302902\n",
      "Epoch 85, loss: 2.302898\n",
      "Epoch 86, loss: 2.301841\n",
      "Epoch 87, loss: 2.301802\n",
      "Epoch 88, loss: 2.302914\n",
      "Epoch 89, loss: 2.301252\n",
      "Epoch 90, loss: 2.301620\n",
      "Epoch 91, loss: 2.302797\n",
      "Epoch 92, loss: 2.302233\n",
      "Epoch 93, loss: 2.302695\n",
      "Epoch 94, loss: 2.302636\n",
      "Epoch 95, loss: 2.302189\n",
      "Epoch 96, loss: 2.303080\n",
      "Epoch 97, loss: 2.302415\n",
      "Epoch 98, loss: 2.301817\n",
      "Epoch 99, loss: 2.300497\n",
      "Epoch 100, loss: 2.302222\n",
      "Epoch 101, loss: 2.300832\n",
      "Epoch 102, loss: 2.302625\n",
      "Epoch 103, loss: 2.302011\n",
      "Epoch 104, loss: 2.302814\n",
      "Epoch 105, loss: 2.302074\n",
      "Epoch 106, loss: 2.301123\n",
      "Epoch 107, loss: 2.302865\n",
      "Epoch 108, loss: 2.301129\n",
      "Epoch 109, loss: 2.301835\n",
      "Epoch 110, loss: 2.302560\n",
      "Epoch 111, loss: 2.302081\n",
      "Epoch 112, loss: 2.302030\n",
      "Epoch 113, loss: 2.302620\n",
      "Epoch 114, loss: 2.302734\n",
      "Epoch 115, loss: 2.302180\n",
      "Epoch 116, loss: 2.302560\n",
      "Epoch 117, loss: 2.302841\n",
      "Epoch 118, loss: 2.301755\n",
      "Epoch 119, loss: 2.301517\n",
      "Epoch 120, loss: 2.302623\n",
      "Epoch 121, loss: 2.301775\n",
      "Epoch 122, loss: 2.303725\n",
      "Epoch 123, loss: 2.301524\n",
      "Epoch 124, loss: 2.302637\n",
      "Epoch 125, loss: 2.302015\n",
      "Epoch 126, loss: 2.302653\n",
      "Epoch 127, loss: 2.302501\n",
      "Epoch 128, loss: 2.300928\n",
      "Epoch 129, loss: 2.301444\n",
      "Epoch 130, loss: 2.303881\n",
      "Epoch 131, loss: 2.302809\n",
      "Epoch 132, loss: 2.302856\n",
      "Epoch 133, loss: 2.301616\n",
      "Epoch 134, loss: 2.301791\n",
      "Epoch 135, loss: 2.301154\n",
      "Epoch 136, loss: 2.303293\n",
      "Epoch 137, loss: 2.301163\n",
      "Epoch 138, loss: 2.302190\n",
      "Epoch 139, loss: 2.302631\n",
      "Epoch 140, loss: 2.302841\n",
      "Epoch 141, loss: 2.302261\n",
      "Epoch 142, loss: 2.302376\n",
      "Epoch 143, loss: 2.301276\n",
      "Epoch 144, loss: 2.302527\n",
      "Epoch 145, loss: 2.302941\n",
      "Epoch 146, loss: 2.300816\n",
      "Epoch 147, loss: 2.302202\n",
      "Epoch 148, loss: 2.303101\n",
      "Epoch 149, loss: 2.302757\n",
      "Epoch 150, loss: 2.302236\n",
      "Epoch 151, loss: 2.303534\n",
      "Epoch 152, loss: 2.301692\n",
      "Epoch 153, loss: 2.302564\n",
      "Epoch 154, loss: 2.302986\n",
      "Epoch 155, loss: 2.301480\n",
      "Epoch 156, loss: 2.301837\n",
      "Epoch 157, loss: 2.302268\n",
      "Epoch 158, loss: 2.303805\n",
      "Epoch 159, loss: 2.301498\n",
      "Epoch 160, loss: 2.303020\n",
      "Epoch 161, loss: 2.302093\n",
      "Epoch 162, loss: 2.302311\n",
      "Epoch 163, loss: 2.300983\n",
      "Epoch 164, loss: 2.301474\n",
      "Epoch 165, loss: 2.301329\n",
      "Epoch 166, loss: 2.300460\n",
      "Epoch 167, loss: 2.301247\n",
      "Epoch 168, loss: 2.301904\n",
      "Epoch 169, loss: 2.302043\n",
      "Epoch 170, loss: 2.301672\n",
      "Epoch 171, loss: 2.302351\n",
      "Epoch 172, loss: 2.302736\n",
      "Epoch 173, loss: 2.301531\n",
      "Epoch 174, loss: 2.302372\n",
      "Epoch 175, loss: 2.303233\n",
      "Epoch 176, loss: 2.301399\n",
      "Epoch 177, loss: 2.301604\n",
      "Epoch 178, loss: 2.302838\n",
      "Epoch 179, loss: 2.301655\n",
      "Epoch 180, loss: 2.301400\n",
      "Epoch 181, loss: 2.302486\n",
      "Epoch 182, loss: 2.300532\n",
      "Epoch 183, loss: 2.302764\n",
      "Epoch 184, loss: 2.301757\n",
      "Epoch 185, loss: 2.301613\n",
      "Epoch 186, loss: 2.302499\n",
      "Epoch 187, loss: 2.301686\n",
      "Epoch 188, loss: 2.301159\n",
      "Epoch 189, loss: 2.301570\n",
      "Epoch 190, loss: 2.301607\n",
      "Epoch 191, loss: 2.302331\n",
      "Epoch 192, loss: 2.302799\n",
      "Epoch 193, loss: 2.302474\n",
      "Epoch 194, loss: 2.302483\n",
      "Epoch 195, loss: 2.302228\n",
      "Epoch 196, loss: 2.301948\n",
      "Epoch 197, loss: 2.301661\n",
      "Epoch 198, loss: 2.302237\n",
      "Epoch 199, loss: 2.301822\n",
      "Epoch 0, loss: 2.302998\n",
      "Epoch 1, loss: 2.301958\n",
      "Epoch 2, loss: 2.301708\n",
      "Epoch 3, loss: 2.303090\n",
      "Epoch 4, loss: 2.302877\n",
      "Epoch 5, loss: 2.302074\n",
      "Epoch 6, loss: 2.301603\n",
      "Epoch 7, loss: 2.303095\n",
      "Epoch 8, loss: 2.303312\n",
      "Epoch 9, loss: 2.302273\n",
      "Epoch 10, loss: 2.301518\n",
      "Epoch 11, loss: 2.302797\n",
      "Epoch 12, loss: 2.302801\n",
      "Epoch 13, loss: 2.304182\n",
      "Epoch 14, loss: 2.302399\n",
      "Epoch 15, loss: 2.302597\n",
      "Epoch 16, loss: 2.301757\n",
      "Epoch 17, loss: 2.302896\n",
      "Epoch 18, loss: 2.302697\n",
      "Epoch 19, loss: 2.303383\n",
      "Epoch 20, loss: 2.302419\n",
      "Epoch 21, loss: 2.301589\n",
      "Epoch 22, loss: 2.302239\n",
      "Epoch 23, loss: 2.302830\n",
      "Epoch 24, loss: 2.301491\n",
      "Epoch 25, loss: 2.301272\n",
      "Epoch 26, loss: 2.301602\n",
      "Epoch 27, loss: 2.301609\n",
      "Epoch 28, loss: 2.302982\n",
      "Epoch 29, loss: 2.302606\n",
      "Epoch 30, loss: 2.303287\n",
      "Epoch 31, loss: 2.302308\n",
      "Epoch 32, loss: 2.302690\n",
      "Epoch 33, loss: 2.302456\n",
      "Epoch 34, loss: 2.301727\n",
      "Epoch 35, loss: 2.302985\n",
      "Epoch 36, loss: 2.302380\n",
      "Epoch 37, loss: 2.303112\n",
      "Epoch 38, loss: 2.303148\n",
      "Epoch 39, loss: 2.301797\n",
      "Epoch 40, loss: 2.302562\n",
      "Epoch 41, loss: 2.302783\n",
      "Epoch 42, loss: 2.303047\n",
      "Epoch 43, loss: 2.303275\n",
      "Epoch 44, loss: 2.302969\n",
      "Epoch 45, loss: 2.302461\n",
      "Epoch 46, loss: 2.303087\n",
      "Epoch 47, loss: 2.302406\n",
      "Epoch 48, loss: 2.303465\n",
      "Epoch 49, loss: 2.303652\n",
      "Epoch 50, loss: 2.301768\n",
      "Epoch 51, loss: 2.303189\n",
      "Epoch 52, loss: 2.302694\n",
      "Epoch 53, loss: 2.302631\n",
      "Epoch 54, loss: 2.303704\n",
      "Epoch 55, loss: 2.302753\n",
      "Epoch 56, loss: 2.301336\n",
      "Epoch 57, loss: 2.302530\n",
      "Epoch 58, loss: 2.302853\n",
      "Epoch 59, loss: 2.301940\n",
      "Epoch 60, loss: 2.302125\n",
      "Epoch 61, loss: 2.301766\n",
      "Epoch 62, loss: 2.303038\n",
      "Epoch 63, loss: 2.302325\n",
      "Epoch 64, loss: 2.302229\n",
      "Epoch 65, loss: 2.300987\n",
      "Epoch 66, loss: 2.302443\n",
      "Epoch 67, loss: 2.302557\n",
      "Epoch 68, loss: 2.303135\n",
      "Epoch 69, loss: 2.303234\n",
      "Epoch 70, loss: 2.302717\n",
      "Epoch 71, loss: 2.303404\n",
      "Epoch 72, loss: 2.303122\n",
      "Epoch 73, loss: 2.301462\n",
      "Epoch 74, loss: 2.302647\n",
      "Epoch 75, loss: 2.303028\n",
      "Epoch 76, loss: 2.302962\n",
      "Epoch 77, loss: 2.303112\n",
      "Epoch 78, loss: 2.302190\n",
      "Epoch 79, loss: 2.302379\n",
      "Epoch 80, loss: 2.302680\n",
      "Epoch 81, loss: 2.302841\n",
      "Epoch 82, loss: 2.302198\n",
      "Epoch 83, loss: 2.301337\n",
      "Epoch 84, loss: 2.302902\n",
      "Epoch 85, loss: 2.303980\n",
      "Epoch 86, loss: 2.302212\n",
      "Epoch 87, loss: 2.301697\n",
      "Epoch 88, loss: 2.303384\n",
      "Epoch 89, loss: 2.302456\n",
      "Epoch 90, loss: 2.302634\n",
      "Epoch 91, loss: 2.302655\n",
      "Epoch 92, loss: 2.301963\n",
      "Epoch 93, loss: 2.302517\n",
      "Epoch 94, loss: 2.302349\n",
      "Epoch 95, loss: 2.303116\n",
      "Epoch 96, loss: 2.301842\n",
      "Epoch 97, loss: 2.302080\n",
      "Epoch 98, loss: 2.303131\n",
      "Epoch 99, loss: 2.301400\n",
      "Epoch 100, loss: 2.302058\n",
      "Epoch 101, loss: 2.302415\n",
      "Epoch 102, loss: 2.302548\n",
      "Epoch 103, loss: 2.302965\n",
      "Epoch 104, loss: 2.302760\n",
      "Epoch 105, loss: 2.302712\n",
      "Epoch 106, loss: 2.302943\n",
      "Epoch 107, loss: 2.301214\n",
      "Epoch 108, loss: 2.302446\n",
      "Epoch 109, loss: 2.302930\n",
      "Epoch 110, loss: 2.301947\n",
      "Epoch 111, loss: 2.302684\n",
      "Epoch 112, loss: 2.301441\n",
      "Epoch 113, loss: 2.302407\n",
      "Epoch 114, loss: 2.303433\n",
      "Epoch 115, loss: 2.303289\n",
      "Epoch 116, loss: 2.303285\n",
      "Epoch 117, loss: 2.301825\n",
      "Epoch 118, loss: 2.302166\n",
      "Epoch 119, loss: 2.302701\n",
      "Epoch 120, loss: 2.302321\n",
      "Epoch 121, loss: 2.301590\n",
      "Epoch 122, loss: 2.302505\n",
      "Epoch 123, loss: 2.302291\n",
      "Epoch 124, loss: 2.302981\n",
      "Epoch 125, loss: 2.302892\n",
      "Epoch 126, loss: 2.303163\n",
      "Epoch 127, loss: 2.301126\n",
      "Epoch 128, loss: 2.304827\n",
      "Epoch 129, loss: 2.302536\n",
      "Epoch 130, loss: 2.303419\n",
      "Epoch 131, loss: 2.301939\n",
      "Epoch 132, loss: 2.302221\n",
      "Epoch 133, loss: 2.302919\n",
      "Epoch 134, loss: 2.302448\n",
      "Epoch 135, loss: 2.302701\n",
      "Epoch 136, loss: 2.301747\n",
      "Epoch 137, loss: 2.302471\n",
      "Epoch 138, loss: 2.303558\n",
      "Epoch 139, loss: 2.302233\n",
      "Epoch 140, loss: 2.303086\n",
      "Epoch 141, loss: 2.302274\n",
      "Epoch 142, loss: 2.301404\n",
      "Epoch 143, loss: 2.302214\n",
      "Epoch 144, loss: 2.302321\n",
      "Epoch 145, loss: 2.302169\n",
      "Epoch 146, loss: 2.302100\n",
      "Epoch 147, loss: 2.302474\n",
      "Epoch 148, loss: 2.302342\n",
      "Epoch 149, loss: 2.302434\n",
      "Epoch 150, loss: 2.302363\n",
      "Epoch 151, loss: 2.301747\n",
      "Epoch 152, loss: 2.302387\n",
      "Epoch 153, loss: 2.302223\n",
      "Epoch 154, loss: 2.302995\n",
      "Epoch 155, loss: 2.302645\n",
      "Epoch 156, loss: 2.302576\n",
      "Epoch 157, loss: 2.302164\n",
      "Epoch 158, loss: 2.302386\n",
      "Epoch 159, loss: 2.303287\n",
      "Epoch 160, loss: 2.302349\n",
      "Epoch 161, loss: 2.303122\n",
      "Epoch 162, loss: 2.301654\n",
      "Epoch 163, loss: 2.302186\n",
      "Epoch 164, loss: 2.301093\n",
      "Epoch 165, loss: 2.302228\n",
      "Epoch 166, loss: 2.301791\n",
      "Epoch 167, loss: 2.302260\n",
      "Epoch 168, loss: 2.303129\n",
      "Epoch 169, loss: 2.302282\n",
      "Epoch 170, loss: 2.302002\n",
      "Epoch 171, loss: 2.303201\n",
      "Epoch 172, loss: 2.302566\n",
      "Epoch 173, loss: 2.302260\n",
      "Epoch 174, loss: 2.301911\n",
      "Epoch 175, loss: 2.300810\n",
      "Epoch 176, loss: 2.302287\n",
      "Epoch 177, loss: 2.302098\n",
      "Epoch 178, loss: 2.302255\n",
      "Epoch 179, loss: 2.301871\n",
      "Epoch 180, loss: 2.301630\n",
      "Epoch 181, loss: 2.303552\n",
      "Epoch 182, loss: 2.302018\n",
      "Epoch 183, loss: 2.302062\n",
      "Epoch 184, loss: 2.303098\n",
      "Epoch 185, loss: 2.303238\n",
      "Epoch 186, loss: 2.301682\n",
      "Epoch 187, loss: 2.303101\n",
      "Epoch 188, loss: 2.302357\n",
      "Epoch 189, loss: 2.303322\n",
      "Epoch 190, loss: 2.301903\n",
      "Epoch 191, loss: 2.301825\n",
      "Epoch 192, loss: 2.302367\n",
      "Epoch 193, loss: 2.302485\n",
      "Epoch 194, loss: 2.302592\n",
      "Epoch 195, loss: 2.302406\n",
      "Epoch 196, loss: 2.302661\n",
      "Epoch 197, loss: 2.302576\n",
      "Epoch 198, loss: 2.301545\n",
      "Epoch 199, loss: 2.302915\n",
      "Epoch 0, loss: 2.302185\n",
      "Epoch 1, loss: 2.302454\n",
      "Epoch 2, loss: 2.303054\n",
      "Epoch 3, loss: 2.302107\n",
      "Epoch 4, loss: 2.303413\n",
      "Epoch 5, loss: 2.302336\n",
      "Epoch 6, loss: 2.302103\n",
      "Epoch 7, loss: 2.301532\n",
      "Epoch 8, loss: 2.302817\n",
      "Epoch 9, loss: 2.303592\n",
      "Epoch 10, loss: 2.301786\n",
      "Epoch 11, loss: 2.302605\n",
      "Epoch 12, loss: 2.302338\n",
      "Epoch 13, loss: 2.303007\n",
      "Epoch 14, loss: 2.301972\n",
      "Epoch 15, loss: 2.303104\n",
      "Epoch 16, loss: 2.301651\n",
      "Epoch 17, loss: 2.302058\n",
      "Epoch 18, loss: 2.302980\n",
      "Epoch 19, loss: 2.301918\n",
      "Epoch 20, loss: 2.302440\n",
      "Epoch 21, loss: 2.303125\n",
      "Epoch 22, loss: 2.303265\n",
      "Epoch 23, loss: 2.301913\n",
      "Epoch 24, loss: 2.303161\n",
      "Epoch 25, loss: 2.301875\n",
      "Epoch 26, loss: 2.302835\n",
      "Epoch 27, loss: 2.302233\n",
      "Epoch 28, loss: 2.303684\n",
      "Epoch 29, loss: 2.302616\n",
      "Epoch 30, loss: 2.302490\n",
      "Epoch 31, loss: 2.301642\n",
      "Epoch 32, loss: 2.302358\n",
      "Epoch 33, loss: 2.302590\n",
      "Epoch 34, loss: 2.303663\n",
      "Epoch 35, loss: 2.302248\n",
      "Epoch 36, loss: 2.303264\n",
      "Epoch 37, loss: 2.302383\n",
      "Epoch 38, loss: 2.301768\n",
      "Epoch 39, loss: 2.301942\n",
      "Epoch 40, loss: 2.302162\n",
      "Epoch 41, loss: 2.301701\n",
      "Epoch 42, loss: 2.301624\n",
      "Epoch 43, loss: 2.303606\n",
      "Epoch 44, loss: 2.302608\n",
      "Epoch 45, loss: 2.302317\n",
      "Epoch 46, loss: 2.301744\n",
      "Epoch 47, loss: 2.303315\n",
      "Epoch 48, loss: 2.302772\n",
      "Epoch 49, loss: 2.301857\n",
      "Epoch 50, loss: 2.302388\n",
      "Epoch 51, loss: 2.303401\n",
      "Epoch 52, loss: 2.303004\n",
      "Epoch 53, loss: 2.303006\n",
      "Epoch 54, loss: 2.302471\n",
      "Epoch 55, loss: 2.302427\n",
      "Epoch 56, loss: 2.302518\n",
      "Epoch 57, loss: 2.303499\n",
      "Epoch 58, loss: 2.302326\n",
      "Epoch 59, loss: 2.302012\n",
      "Epoch 60, loss: 2.302223\n",
      "Epoch 61, loss: 2.302477\n",
      "Epoch 62, loss: 2.301039\n",
      "Epoch 63, loss: 2.302784\n",
      "Epoch 64, loss: 2.302379\n",
      "Epoch 65, loss: 2.302389\n",
      "Epoch 66, loss: 2.303621\n",
      "Epoch 67, loss: 2.302598\n",
      "Epoch 68, loss: 2.302769\n",
      "Epoch 69, loss: 2.302676\n",
      "Epoch 70, loss: 2.301698\n",
      "Epoch 71, loss: 2.303013\n",
      "Epoch 72, loss: 2.302703\n",
      "Epoch 73, loss: 2.302130\n",
      "Epoch 74, loss: 2.302503\n",
      "Epoch 75, loss: 2.301584\n",
      "Epoch 76, loss: 2.302607\n",
      "Epoch 77, loss: 2.302021\n",
      "Epoch 78, loss: 2.303051\n",
      "Epoch 79, loss: 2.303005\n",
      "Epoch 80, loss: 2.302703\n",
      "Epoch 81, loss: 2.301859\n",
      "Epoch 82, loss: 2.302519\n",
      "Epoch 83, loss: 2.303116\n",
      "Epoch 84, loss: 2.302725\n",
      "Epoch 85, loss: 2.302789\n",
      "Epoch 86, loss: 2.301735\n",
      "Epoch 87, loss: 2.302028\n",
      "Epoch 88, loss: 2.301756\n",
      "Epoch 89, loss: 2.302981\n",
      "Epoch 90, loss: 2.302006\n",
      "Epoch 91, loss: 2.301725\n",
      "Epoch 92, loss: 2.301717\n",
      "Epoch 93, loss: 2.302179\n",
      "Epoch 94, loss: 2.303140\n",
      "Epoch 95, loss: 2.303161\n",
      "Epoch 96, loss: 2.302828\n",
      "Epoch 97, loss: 2.304063\n",
      "Epoch 98, loss: 2.300860\n",
      "Epoch 99, loss: 2.303427\n",
      "Epoch 100, loss: 2.301627\n",
      "Epoch 101, loss: 2.302731\n",
      "Epoch 102, loss: 2.302624\n",
      "Epoch 103, loss: 2.302222\n",
      "Epoch 104, loss: 2.302942\n",
      "Epoch 105, loss: 2.302258\n",
      "Epoch 106, loss: 2.302962\n",
      "Epoch 107, loss: 2.303046\n",
      "Epoch 108, loss: 2.303170\n",
      "Epoch 109, loss: 2.303160\n",
      "Epoch 110, loss: 2.302185\n",
      "Epoch 111, loss: 2.302604\n",
      "Epoch 112, loss: 2.302301\n",
      "Epoch 113, loss: 2.302269\n",
      "Epoch 114, loss: 2.301503\n",
      "Epoch 115, loss: 2.301989\n",
      "Epoch 116, loss: 2.302343\n",
      "Epoch 117, loss: 2.302476\n",
      "Epoch 118, loss: 2.301531\n",
      "Epoch 119, loss: 2.302096\n",
      "Epoch 120, loss: 2.302555\n",
      "Epoch 121, loss: 2.301332\n",
      "Epoch 122, loss: 2.301631\n",
      "Epoch 123, loss: 2.301846\n",
      "Epoch 124, loss: 2.302159\n",
      "Epoch 125, loss: 2.301679\n",
      "Epoch 126, loss: 2.302035\n",
      "Epoch 127, loss: 2.302512\n",
      "Epoch 128, loss: 2.302652\n",
      "Epoch 129, loss: 2.301876\n",
      "Epoch 130, loss: 2.302590\n",
      "Epoch 131, loss: 2.302181\n",
      "Epoch 132, loss: 2.301862\n",
      "Epoch 133, loss: 2.301814\n",
      "Epoch 134, loss: 2.302396\n",
      "Epoch 135, loss: 2.301948\n",
      "Epoch 136, loss: 2.302600\n",
      "Epoch 137, loss: 2.302201\n",
      "Epoch 138, loss: 2.301917\n",
      "Epoch 139, loss: 2.301783\n",
      "Epoch 140, loss: 2.302868\n",
      "Epoch 141, loss: 2.302402\n",
      "Epoch 142, loss: 2.302597\n",
      "Epoch 143, loss: 2.301833\n",
      "Epoch 144, loss: 2.302616\n",
      "Epoch 145, loss: 2.302287\n",
      "Epoch 146, loss: 2.302579\n",
      "Epoch 147, loss: 2.302132\n",
      "Epoch 148, loss: 2.301943\n",
      "Epoch 149, loss: 2.302239\n",
      "Epoch 150, loss: 2.300738\n",
      "Epoch 151, loss: 2.301582\n",
      "Epoch 152, loss: 2.302875\n",
      "Epoch 153, loss: 2.302868\n",
      "Epoch 154, loss: 2.302142\n",
      "Epoch 155, loss: 2.303319\n",
      "Epoch 156, loss: 2.302928\n",
      "Epoch 157, loss: 2.301877\n",
      "Epoch 158, loss: 2.301696\n",
      "Epoch 159, loss: 2.302931\n",
      "Epoch 160, loss: 2.302055\n",
      "Epoch 161, loss: 2.302260\n",
      "Epoch 162, loss: 2.301645\n",
      "Epoch 163, loss: 2.302558\n",
      "Epoch 164, loss: 2.301400\n",
      "Epoch 165, loss: 2.302740\n",
      "Epoch 166, loss: 2.302478\n",
      "Epoch 167, loss: 2.302083\n",
      "Epoch 168, loss: 2.302064\n",
      "Epoch 169, loss: 2.302052\n",
      "Epoch 170, loss: 2.303399\n",
      "Epoch 171, loss: 2.302617\n",
      "Epoch 172, loss: 2.302983\n",
      "Epoch 173, loss: 2.302569\n",
      "Epoch 174, loss: 2.302607\n",
      "Epoch 175, loss: 2.302377\n",
      "Epoch 176, loss: 2.302812\n",
      "Epoch 177, loss: 2.302830\n",
      "Epoch 178, loss: 2.301889\n",
      "Epoch 179, loss: 2.303264\n",
      "Epoch 180, loss: 2.302653\n",
      "Epoch 181, loss: 2.302602\n",
      "Epoch 182, loss: 2.302989\n",
      "Epoch 183, loss: 2.301581\n",
      "Epoch 184, loss: 2.302187\n",
      "Epoch 185, loss: 2.303864\n",
      "Epoch 186, loss: 2.302431\n",
      "Epoch 187, loss: 2.301461\n",
      "Epoch 188, loss: 2.302270\n",
      "Epoch 189, loss: 2.302522\n",
      "Epoch 190, loss: 2.303464\n",
      "Epoch 191, loss: 2.302217\n",
      "Epoch 192, loss: 2.300690\n",
      "Epoch 193, loss: 2.302252\n",
      "Epoch 194, loss: 2.301932\n",
      "Epoch 195, loss: 2.301924\n",
      "Epoch 196, loss: 2.301890\n",
      "Epoch 197, loss: 2.302296\n",
      "Epoch 198, loss: 2.302217\n",
      "Epoch 199, loss: 2.302555\n",
      "Epoch 0, loss: 2.302625\n",
      "Epoch 1, loss: 2.302281\n",
      "Epoch 2, loss: 2.301771\n",
      "Epoch 3, loss: 2.300429\n",
      "Epoch 4, loss: 2.302426\n",
      "Epoch 5, loss: 2.303498\n",
      "Epoch 6, loss: 2.303093\n",
      "Epoch 7, loss: 2.301974\n",
      "Epoch 8, loss: 2.301400\n",
      "Epoch 9, loss: 2.303334\n",
      "Epoch 10, loss: 2.302220\n",
      "Epoch 11, loss: 2.302606\n",
      "Epoch 12, loss: 2.302830\n",
      "Epoch 13, loss: 2.303274\n",
      "Epoch 14, loss: 2.302714\n",
      "Epoch 15, loss: 2.302685\n",
      "Epoch 16, loss: 2.301917\n",
      "Epoch 17, loss: 2.302157\n",
      "Epoch 18, loss: 2.302457\n",
      "Epoch 19, loss: 2.303591\n",
      "Epoch 20, loss: 2.302267\n",
      "Epoch 21, loss: 2.302598\n",
      "Epoch 22, loss: 2.302578\n",
      "Epoch 23, loss: 2.301752\n",
      "Epoch 24, loss: 2.302812\n",
      "Epoch 25, loss: 2.302905\n",
      "Epoch 26, loss: 2.301915\n",
      "Epoch 27, loss: 2.302103\n",
      "Epoch 28, loss: 2.302305\n",
      "Epoch 29, loss: 2.302503\n",
      "Epoch 30, loss: 2.302779\n",
      "Epoch 31, loss: 2.302679\n",
      "Epoch 32, loss: 2.301625\n",
      "Epoch 33, loss: 2.302019\n",
      "Epoch 34, loss: 2.302029\n",
      "Epoch 35, loss: 2.301971\n",
      "Epoch 36, loss: 2.301874\n",
      "Epoch 37, loss: 2.302305\n",
      "Epoch 38, loss: 2.302571\n",
      "Epoch 39, loss: 2.302491\n",
      "Epoch 40, loss: 2.303119\n",
      "Epoch 41, loss: 2.301920\n",
      "Epoch 42, loss: 2.302273\n",
      "Epoch 43, loss: 2.303247\n",
      "Epoch 44, loss: 2.301733\n",
      "Epoch 45, loss: 2.301925\n",
      "Epoch 46, loss: 2.302327\n",
      "Epoch 47, loss: 2.301921\n",
      "Epoch 48, loss: 2.302061\n",
      "Epoch 49, loss: 2.301199\n",
      "Epoch 50, loss: 2.303399\n",
      "Epoch 51, loss: 2.301918\n",
      "Epoch 52, loss: 2.303357\n",
      "Epoch 53, loss: 2.302600\n",
      "Epoch 54, loss: 2.302240\n",
      "Epoch 55, loss: 2.302291\n",
      "Epoch 56, loss: 2.302450\n",
      "Epoch 57, loss: 2.302264\n",
      "Epoch 58, loss: 2.302386\n",
      "Epoch 59, loss: 2.302045\n",
      "Epoch 60, loss: 2.303221\n",
      "Epoch 61, loss: 2.302407\n",
      "Epoch 62, loss: 2.301585\n",
      "Epoch 63, loss: 2.302186\n",
      "Epoch 64, loss: 2.302472\n",
      "Epoch 65, loss: 2.302658\n",
      "Epoch 66, loss: 2.302832\n",
      "Epoch 67, loss: 2.302248\n",
      "Epoch 68, loss: 2.302266\n",
      "Epoch 69, loss: 2.302442\n",
      "Epoch 70, loss: 2.301217\n",
      "Epoch 71, loss: 2.301810\n",
      "Epoch 72, loss: 2.302400\n",
      "Epoch 73, loss: 2.302817\n",
      "Epoch 74, loss: 2.303257\n",
      "Epoch 75, loss: 2.301562\n",
      "Epoch 76, loss: 2.303234\n",
      "Epoch 77, loss: 2.303096\n",
      "Epoch 78, loss: 2.302993\n",
      "Epoch 79, loss: 2.302904\n",
      "Epoch 80, loss: 2.302217\n",
      "Epoch 81, loss: 2.301846\n",
      "Epoch 82, loss: 2.302403\n",
      "Epoch 83, loss: 2.302734\n",
      "Epoch 84, loss: 2.302427\n",
      "Epoch 85, loss: 2.301826\n",
      "Epoch 86, loss: 2.301941\n",
      "Epoch 87, loss: 2.301561\n",
      "Epoch 88, loss: 2.302263\n",
      "Epoch 89, loss: 2.302770\n",
      "Epoch 90, loss: 2.303348\n",
      "Epoch 91, loss: 2.303073\n",
      "Epoch 92, loss: 2.302030\n",
      "Epoch 93, loss: 2.302192\n",
      "Epoch 94, loss: 2.303186\n",
      "Epoch 95, loss: 2.301692\n",
      "Epoch 96, loss: 2.302375\n",
      "Epoch 97, loss: 2.301775\n",
      "Epoch 98, loss: 2.302757\n",
      "Epoch 99, loss: 2.302466\n",
      "Epoch 100, loss: 2.302354\n",
      "Epoch 101, loss: 2.303010\n",
      "Epoch 102, loss: 2.302987\n",
      "Epoch 103, loss: 2.302621\n",
      "Epoch 104, loss: 2.302671\n",
      "Epoch 105, loss: 2.301640\n",
      "Epoch 106, loss: 2.303221\n",
      "Epoch 107, loss: 2.301539\n",
      "Epoch 108, loss: 2.301565\n",
      "Epoch 109, loss: 2.302734\n",
      "Epoch 110, loss: 2.302201\n",
      "Epoch 111, loss: 2.302820\n",
      "Epoch 112, loss: 2.302913\n",
      "Epoch 113, loss: 2.302116\n",
      "Epoch 114, loss: 2.301868\n",
      "Epoch 115, loss: 2.302855\n",
      "Epoch 116, loss: 2.302331\n",
      "Epoch 117, loss: 2.301513\n",
      "Epoch 118, loss: 2.302668\n",
      "Epoch 119, loss: 2.302519\n",
      "Epoch 120, loss: 2.303033\n",
      "Epoch 121, loss: 2.301645\n",
      "Epoch 122, loss: 2.303084\n",
      "Epoch 123, loss: 2.301940\n",
      "Epoch 124, loss: 2.303693\n",
      "Epoch 125, loss: 2.302733\n",
      "Epoch 126, loss: 2.302529\n",
      "Epoch 127, loss: 2.302017\n",
      "Epoch 128, loss: 2.301730\n",
      "Epoch 129, loss: 2.302123\n",
      "Epoch 130, loss: 2.301853\n",
      "Epoch 131, loss: 2.301786\n",
      "Epoch 132, loss: 2.301724\n",
      "Epoch 133, loss: 2.302464\n",
      "Epoch 134, loss: 2.302492\n",
      "Epoch 135, loss: 2.302091\n",
      "Epoch 136, loss: 2.302394\n",
      "Epoch 137, loss: 2.302468\n",
      "Epoch 138, loss: 2.302252\n",
      "Epoch 139, loss: 2.302754\n",
      "Epoch 140, loss: 2.302256\n",
      "Epoch 141, loss: 2.302350\n",
      "Epoch 142, loss: 2.302577\n",
      "Epoch 143, loss: 2.302448\n",
      "Epoch 144, loss: 2.302184\n",
      "Epoch 145, loss: 2.302736\n",
      "Epoch 146, loss: 2.301959\n",
      "Epoch 147, loss: 2.302122\n",
      "Epoch 148, loss: 2.303213\n",
      "Epoch 149, loss: 2.302579\n",
      "Epoch 150, loss: 2.302488\n",
      "Epoch 151, loss: 2.302219\n",
      "Epoch 152, loss: 2.302439\n",
      "Epoch 153, loss: 2.303046\n",
      "Epoch 154, loss: 2.301743\n",
      "Epoch 155, loss: 2.301930\n",
      "Epoch 156, loss: 2.301943\n",
      "Epoch 157, loss: 2.302468\n",
      "Epoch 158, loss: 2.302383\n",
      "Epoch 159, loss: 2.301605\n",
      "Epoch 160, loss: 2.301807\n",
      "Epoch 161, loss: 2.302675\n",
      "Epoch 162, loss: 2.302715\n",
      "Epoch 163, loss: 2.301529\n",
      "Epoch 164, loss: 2.302820\n",
      "Epoch 165, loss: 2.302764\n",
      "Epoch 166, loss: 2.301577\n",
      "Epoch 167, loss: 2.301233\n",
      "Epoch 168, loss: 2.302790\n",
      "Epoch 169, loss: 2.302222\n",
      "Epoch 170, loss: 2.301833\n",
      "Epoch 171, loss: 2.301212\n",
      "Epoch 172, loss: 2.302924\n",
      "Epoch 173, loss: 2.302875\n",
      "Epoch 174, loss: 2.302090\n",
      "Epoch 175, loss: 2.302753\n",
      "Epoch 176, loss: 2.302492\n",
      "Epoch 177, loss: 2.302018\n",
      "Epoch 178, loss: 2.302219\n",
      "Epoch 179, loss: 2.302945\n",
      "Epoch 180, loss: 2.302377\n",
      "Epoch 181, loss: 2.302597\n",
      "Epoch 182, loss: 2.302974\n",
      "Epoch 183, loss: 2.301281\n",
      "Epoch 184, loss: 2.302195\n",
      "Epoch 185, loss: 2.301816\n",
      "Epoch 186, loss: 2.302423\n",
      "Epoch 187, loss: 2.302801\n",
      "Epoch 188, loss: 2.301434\n",
      "Epoch 189, loss: 2.301825\n",
      "Epoch 190, loss: 2.303292\n",
      "Epoch 191, loss: 2.302769\n",
      "Epoch 192, loss: 2.301719\n",
      "Epoch 193, loss: 2.302846\n",
      "Epoch 194, loss: 2.301767\n",
      "Epoch 195, loss: 2.302680\n",
      "Epoch 196, loss: 2.302144\n",
      "Epoch 197, loss: 2.301585\n",
      "Epoch 198, loss: 2.302735\n",
      "Epoch 199, loss: 2.302764\n",
      "best validation accuracy achieved: 0.152000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "for lr in learning_rates:\n",
    "    for reg in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=lr, batch_size=batch_size, reg=reg)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        if best_val_accuracy is None or accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            best_classifier = classifier\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.123000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}